{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOTxPs4FqrMVtdFQdu2K4j"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wOXgLjQnWsid"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "html = urlopen(\"https://en.wikipedia.org/wiki/Kevin_Bacon\")"
      ],
      "metadata": {
        "id": "YPFwi4V6XqaR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = BeautifulSoup(html, 'html.parser')"
      ],
      "metadata": {
        "id": "y7Stv7OoXxN7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##extract all link in page:"
      ],
      "metadata": {
        "id": "i1Hd2v03tHvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for link in bs.find_all('a'):\n",
        "  if 'href' in link.attrs:\n",
        "    print(link.attrs['href'])"
      ],
      "metadata": {
        "id": "Nkbnyr6EX3wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## extract only internal links:"
      ],
      "metadata": {
        "id": "tVBOOgVXtSCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for link in bs.find('div', {'id': 'bodyContent'}).find_all('a', href = re.compile('^(/wiki/)((?!:).)*$')):\n",
        "  if 'href' in link.attrs:\n",
        "    print(link.attrs['href'])"
      ],
      "metadata": {
        "id": "OSDoftmhYOfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create a script that takes a link and return all internal links and so on until he finds no links:"
      ],
      "metadata": {
        "id": "cDFQN4NHtdwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import random"
      ],
      "metadata": {
        "id": "TOVcoAl-aN6k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(datetime.datetime.now())"
      ],
      "metadata": {
        "id": "Mqk90t8ZaWrG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getLinks(articleUrl):\n",
        "  html = urlopen('http://en.wikipedia.org{}'.format(articleUrl))\n",
        "  bs = BeautifulSoup(html, 'html.parser')\n",
        "  return bs.find('div', {'id': 'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))"
      ],
      "metadata": {
        "id": "z_Hi-J8Nabk4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links = getLinks('/wiki/Kevin_Bacon')"
      ],
      "metadata": {
        "id": "8hjbIaz0bLGB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while len(links) > 0:\n",
        "  newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
        "  print(newArticle)\n",
        "  links = getLinks(newArticle)"
      ],
      "metadata": {
        "id": "XUkVmGubapGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## crowling entire website: not page should be visited twice:"
      ],
      "metadata": {
        "id": "aIvkclTGto_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages =  set()\n",
        "def getLinks(pageUrl):\n",
        "  global pages\n",
        "  html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n",
        "  bs = BeautifulSoup(html, 'html.parser')\n",
        "  for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
        "    if 'href' in link.attrs:\n",
        "      if link.attrs['href'] not in pages:\n",
        "        #we have encountred a new page\n",
        "        newPage = link.attrs['href']\n",
        "        print(newPage)\n",
        "        pages.add(newPage)\n",
        "        getLinks(newPage)\n",
        "getLinks('')\n",
        "#this script will crash whem >> 1000 recursion"
      ],
      "metadata": {
        "id": "34PcWi_HktF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## collecting data across entire site:"
      ],
      "metadata": {
        "id": "LaYrmD0Dtuva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages = set()\n",
        "def getLinks(pageUrl):\n",
        "  global pages\n",
        "  html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n",
        "  bs = BeautifulSoup(html, 'html.parser')\n",
        "  try:\n",
        "    #for everthe y page get the title, the first patagraph and the edit page if exist! \n",
        "    print(bs.h1.get_text())\n",
        "    print(bs.find(id ='mw-content-text').find_all('p')[0])\n",
        "    print(bs.find(id='ca-edit').find('span').find('a').attrs['href'])\n",
        "  except AttributeError:\n",
        "    print('This page is missing something! Continuing.')\n",
        "\n",
        "  for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
        "    if 'href' in link.attrs:\n",
        "      if link.attrs['href'] not in pages:\n",
        "        #We have encountered a new page\n",
        "        newPage = link.attrs['href']\n",
        "        print('-'*20)\n",
        "        print(newPage)\n",
        "        pages.add(newPage)\n",
        "        getLinks(newPage)\n",
        "getLinks('')"
      ],
      "metadata": {
        "id": "I3LX10GJnVp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *handling redirection the urllib handels it automaticly for requests must allow it !!!*"
      ],
      "metadata": {
        "id": "xJ1rSpC2t0sD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## this script get internal links and looks for externals ones ..."
      ],
      "metadata": {
        "id": "9JeMaTGFuF-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages = set()\n",
        "random.seed(datetime.datetime.now())\n",
        "#Retrieves a list of all Internal links found on a page\n",
        "def getInternalLinks(bs, includeUrl):\n",
        "  includeUrl = '{}://{}'.format(urlparse(includeUrl).scheme,\n",
        "  urlparse(includeUrl).netloc)\n",
        "  internalLinks = []\n",
        "  #Finds all links that begin with a \"/\"\n",
        "  for link in bs.find_all('a',href=re.compile('^(/|.*'+includeUrl+')')):\n",
        "    if link.attrs['href'] is not None:\n",
        "      if link.attrs['href'] not in internalLinks:\n",
        "        if(link.attrs['href'].startswith('/')):\n",
        "          internalLinks.append(includeUrl+link.attrs['href'])\n",
        "        else:\n",
        "          internalLinks.append(link.attrs['href'])\n",
        "  return internalLinks\n",
        "\n",
        "#Retrieves a list of all external links found on a page\n",
        "def getExternalLinks(bs, excludeUrl):\n",
        "  externalLinks = []\n",
        "  #Finds all links that start with \"http\" that do\n",
        "  #not contain the current URL\n",
        "  for link in bs.find_all('a',href=re.compile('^(http|www)((?!'+excludeUrl+').)*$')):\n",
        "    if link.attrs['href'] is not None:\n",
        "      if link.attrs['href'] not in externalLinks:\n",
        "        externalLinks.append(link.attrs['href'])\n",
        "  return externalLinks\n",
        "\n",
        "def getRandomExternalLink(startingPage):\n",
        "  html = urlopen(startingPage)\n",
        "  bs = BeautifulSoup(html, 'html.parser')\n",
        "  externalLinks = getExternalLinks(bs,\n",
        "  urlparse(startingPage).netloc)\n",
        "  if len(externalLinks) == 0:\n",
        "    print('No external links, looking around the site for one')\n",
        "    domain = '{}://{}'.format(urlparse(startingPage).scheme,urlparse(startingPage).netloc)\n",
        "    internalLinks = getInternalLinks(bs, domain)\n",
        "    return getRandomExternalLink(internalLinks[random.randint(0,len(internalLinks)-1)])\n",
        "  else:\n",
        "    return externalLinks[random.randint(0, len(externalLinks)-1)]\n",
        "\n",
        "def followExternalOnly(startingSite):\n",
        "  externalLink = getRandomExternalLink(startingSite)\n",
        "  print('Random external link is: {}'.format(externalLink))\n",
        "  followExternalOnly(externalLink)\n",
        "\n",
        "followExternalOnly('http://oreilly.com')"
      ],
      "metadata": {
        "id": "JMv9MQojqE2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## crowl website for external links:"
      ],
      "metadata": {
        "id": "L8vjCUM4uMV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collects a list of all external URLs found on the site\n",
        "allExtLinks = set()\n",
        "allIntLinks = set()\n",
        "def getAllExternalLinks(siteUrl):\n",
        "  html = urlopen(siteUrl)\n",
        "  domain = '{}://{}'.format(urlparse(siteUrl).scheme,urlparse(siteUrl).netloc)\n",
        "  bs = BeautifulSoup(html, 'html.parser')\n",
        "  internalLinks = getInternalLinks(bs, domain)\n",
        "  externalLinks = getExternalLinks(bs, domain)\n",
        "  for link in externalLinks:\n",
        "    if link not in allExtLinks:\n",
        "      allExtLinks.add(link)\n",
        "      print(link)\n",
        "  for link in internalLinks:\n",
        "    if link not in allIntLinks:\n",
        "      allIntLinks.add(link)\n",
        "      getAllExternalLinks(link)\n",
        "\n",
        "alIntLinks.add('http://oreilly.com')\n",
        "getAllExternalLinks('http://oreilly.com') "
      ],
      "metadata": {
        "id": "A9cBQLqfrxvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n9aXw8HjsnBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}